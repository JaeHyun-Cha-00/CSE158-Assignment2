{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataVer10 = \"review-Vermont-10.json.gz\"\n",
        "dataVer = \"review-Vermont.json.gz\"\n",
        "dataMeta = \"meta-Vermont.json.gz\"\n",
        "\n",
        "df_meta = pd.read_json(dataMeta, lines=True, compression=\"gzip\")\n",
        "df_ver = pd.read_json(dataVer, lines=True, compression=\"gzip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "HgjPAwlhf4bK",
        "outputId": "ce9027be-97c1-4fdc-e2e6-0bd2a803d032"
      },
      "outputs": [],
      "source": [
        "#merge into one dataframe\n",
        "df = df_ver.merge(df_meta, on=\"gmap_id\", how=\"left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3779AsMbgRvy"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#remove duplicates user_id and gmap_id remove user reviewing a place multiple times\n",
        "print(\"Before:\", len(df))\n",
        "df = df.drop_duplicates(subset=['user_id', 'gmap_id'])\n",
        "print(\"After:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#drop any with none rating\n",
        "print(\"Before:\", len(df))\n",
        "df = df.dropna(subset=['rating'])\n",
        "print(\"After:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#drop any with none text reviews\n",
        "print(\"Before:\", len(df))\n",
        "df = df.dropna(subset=[\"text\"])\n",
        "print(\"After:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#number of reviews recalcualte\n",
        "num_reviews = df.groupby('gmap_id')['rating'].count().rename('num_of_reviews')\n",
        "\n",
        "#average rating recalculate\n",
        "avg_rating = df.groupby('gmap_id')['rating'].mean().rename('avg_rating')\n",
        "\n",
        "# combine back into dataframe\n",
        "df = df.drop(columns=['num_of_reviews','avg_rating'], errors='ignore')\n",
        "df = df.merge(num_reviews, on='gmap_id', how='left')\n",
        "df = df.merge(avg_rating, on='gmap_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#enconding of pricing convert $$$ price to numerical: 0 = none, 1 = $, 2 = $$, 3 = $$$, 4 = $$$$\n",
        "df[\"price\"] = df[\"price\"].str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction import FeatureHasher\n",
        "\n",
        "# Ensure all entries are lists (empty if missing) nan fix\n",
        "df['category'] = df['category'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Create the hasher\n",
        "hasher = FeatureHasher(n_features=20, input_type='string')\n",
        "\n",
        "# Transform the column\n",
        "hashed_features = hasher.transform(df['category'])\n",
        "\n",
        "# Convert sparse matrix to dense array\n",
        "hashed_array = hashed_features.toarray()\n",
        "\n",
        "# Make a DataFrame with column names\n",
        "hashed_df = pd.DataFrame(hashed_array, columns=[f\"cat_{i}\" for i in range(hashed_array.shape[1])])\n",
        "\n",
        "#merge back with dataframe\n",
        "df = pd.concat([df.reset_index(drop=True), hashed_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#natural numerical categories\n",
        "\n",
        "cathash_cols = [f\"cat_{i}\" for i in range(20)]\n",
        "numerical_cols = [\"rating\",\"avg_rating\",\"price\", \"num_of_reviews\",\"longitude\",\"latitude\",\"time\"] \n",
        "\n",
        "all_cols = numerical_cols + cathash_cols\n",
        "\n",
        "#correlation matrix \n",
        "corr = df[all_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(25,15))\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "ax = sns.histplot(data=df, x=\"rating\", bins=5, kde=False)\n",
        "\n",
        "# Title and labels\n",
        "plt.title(\"Distribution of Ratings\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "total = len(df)\n",
        "\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    percent = 100 * height / total\n",
        "    x = p.get_x() + p.get_width() / 2\n",
        "    y = height\n",
        "\n",
        "    ax.annotate(f\"{percent:.1f}%\", (x, y), ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "ax = sns.histplot(data=df, x=\"avg_rating\", bins=5, kde=False)\n",
        "\n",
        "plt.title(\"Distribution of average Business Ratings\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "total = len(df)\n",
        "\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    percent = 100 * height / total\n",
        "    x = p.get_x() + p.get_width() / 2\n",
        "    y = height\n",
        "\n",
        "    ax.annotate(f\"{percent:.1f}%\", (x, y), ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"state\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from textblob import TextBlob\n",
        "# import numpy as np\n",
        "\n",
        "# #feature engineering\n",
        "\n",
        "# df[\"len_chars\"] = df[\"text\"].str.len()\n",
        "# df[\"len_words\"] = df[\"text\"].str.split().str.len()\n",
        "\n",
        "# def blob_features(text):\n",
        "#     b = TextBlob(text)\n",
        "#     return pd.Series([b.sentiment.polarity, b.sentiment.subjectivity])\n",
        "\n",
        "# df[[\"polarity\", \"subjectivity\"]] = df[\"text\"].apply(blob_features)\n",
        "\n",
        "\n",
        "# # Feature + label\n",
        "# X_text = df['text']\n",
        "# # other features \n",
        "# X_extra = df[[\"time\", \"price\", \"num_of_reviews\", \"longitude\", \n",
        "#               \"latitude\", \"subjectivity\", \"polarity\"] + cathash_cols]\n",
        "# y = df['rating']\n",
        "\n",
        "# # 3. Train/Val split\n",
        "# X_train_text, X_test_text, X_train_extra, X_test_extra, y_train, y_test = train_test_split(\n",
        "#     X_text, X_extra, y,\n",
        "#     test_size=0.2,\n",
        "#     random_state=42,\n",
        "#     stratify=y\n",
        "# )\n",
        "\n",
        "# # 4. TF-IDF (baseline)\n",
        "# tfidf = TfidfVectorizer(\n",
        "#     max_features=50000,\n",
        "#     ngram_range=(1, 2),\n",
        "#     min_df=2,\n",
        "#     max_df=0.95\n",
        "# )\n",
        "\n",
        "# X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "# X_test_tfidf = tfidf.transform(X_test_text)\n",
        "\n",
        "# #predict text TF-IDF\n",
        "# model_text = Ridge(alpha=1.0)\n",
        "# model_text.fit(X_train_tfidf, y_train)\n",
        "# y_pred_text = model_text.predict(X_test_tfidf)\n",
        "\n",
        "# # numeric_cols = [\"time\", \"num_of_reviews\", \"longitude\", \"latitude\", \"subjectivity\", \"polarity\"] + cathash_cols\n",
        "# scaler = StandardScaler()\n",
        "# X_train_numeric_scaled = scaler.fit_transform(X_train_extra[cathash_cols])\n",
        "# X_test_numeric_scaled = scaler.transform(X_test_extra[cathash_cols])\n",
        "\n",
        "# #predict numerical\n",
        "# model_numeric = Ridge(alpha=1.0)\n",
        "# model_numeric.fit(X_train_numeric_scaled, y_train)\n",
        "# y_pred_numeric = model_numeric.predict(X_test_numeric_scaled)\n",
        "\n",
        "# y_pred_final = 0.7*y_pred_text + 0.3*y_pred_numeric\n",
        "# y_pred_final = np.clip(np.round(y_pred_final), 1, 5)\n",
        "\n",
        "# print(\"Accuracy:\", accuracy_score(y_test, y_pred_final))\n",
        "# mse = mean_squared_error(y_test, y_pred_final)\n",
        "# print(\"MSE:\", mse)\n",
        "# rmse = np.sqrt(mse)\n",
        "# print(\"RMSE:\", rmse)\n",
        "\n",
        "\n",
        "# # Accuracy: 0.647806087428671\n",
        "# # MSE: 0.5231889848480191\n",
        "# # RMSE: 0.7233180385197228"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# useless = [\"name_x\", \"name_y\", \"time\", \"pics\", \"resp\", \"address\", \"relative_results\", \"state\", \"url\", \"latitude\", \"longitude\", \"num_of_reviews\"]\n",
        "# maybe = [\"description\"]\n",
        "# df = df.drop(columns=useless + maybe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.to_json(\"clean_Vermont.json.gz\", orient=\"records\", lines=True, compression=\"gzip\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
