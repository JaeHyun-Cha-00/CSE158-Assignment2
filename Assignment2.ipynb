{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LSQll_SFtzN"
      },
      "outputs": [],
      "source": [
        "# All imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments, default_data_collator\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "\n",
        "# Set style for publication-quality figures\n",
        "plt.style.use('seaborn-v0_8-paper')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "plt.rcParams['figure.titlesize'] = 16\n",
        "\n",
        "# Load data\n",
        "dataVer = \"review-Vermont_10.json.gz\"\n",
        "dataMeta = \"meta-Vermont.json.gz\"\n",
        "\n",
        "df_meta = pd.read_json(dataMeta, lines=True, compression=\"gzip\")\n",
        "df_ver = pd.read_json(dataVer, lines=True, compression=\"gzip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgjPAwlhf4bK",
        "outputId": "d13f7d37-8aa5-4486-db9a-7188773964e4"
      },
      "outputs": [],
      "source": [
        "# Merge review and metadata dataframes\n",
        "df = df_ver.merge(df_meta, on=\"gmap_id\", how=\"left\")\n",
        "\n",
        "# Remove rows with missing text\n",
        "print(\"Before removing missing text:\", len(df))\n",
        "df = df.dropna(subset=[\"text\"])\n",
        "print(\"After removing missing text:\", len(df))\n",
        "\n",
        "# Remove duplicate reviews (same user reviewing same business)\n",
        "print(\"Before removing duplicates:\", len(df))\n",
        "df = df.drop_duplicates(subset=['user_id', 'gmap_id'])\n",
        "print(\"After removing duplicates:\", len(df))\n",
        "\n",
        "# Remove unnecessary columns\n",
        "useless = [\"name_x\", \"name_y\", \"time\", \"pics\", \"resp\", \"address\",\n",
        "           \"relative_results\", \"state\", \"url\", \"latitude\", \"longitude\", \"MISC\"]\n",
        "maybe = [\"description\"] # num_of_reviews\n",
        "df = df.drop(columns=useless + maybe)\n",
        "\n",
        "# Remove rows with missing ratings\n",
        "df = df.dropna(subset=['rating'])\n",
        "print(\"After removing missing ratings:\", len(df))\n",
        "\n",
        "# Save preprocessed data\n",
        "df.to_csv(\"merged.csv\", index=False)\n",
        "print(\"Preprocessed data saved to merged.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "3779AsMbgRvy",
        "outputId": "ee0419b3-d32a-4898-cf5c-a06eef57e2ec"
      },
      "outputs": [],
      "source": [
        "# Display sample of preprocessed data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTOs_RmlFtzQ"
      },
      "source": [
        "# II. Data Exploration and Visualization\n",
        "\n",
        "This section provides visualizations and statistical analysis of the dataset to understand the data distribution and relationships between features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlT3UhLUFtzR",
        "outputId": "24085084-9f23-4138-e988-9029261a5fe9"
      },
      "outputs": [],
      "source": [
        "# Load preprocessed data for visualization\n",
        "df_viz = pd.read_csv(\"merged.csv\")\n",
        "\n",
        "# Basic statistics\n",
        "print(\"Dataset Statistics:\")\n",
        "print(f\"Total reviews: {len(df_viz):,}\")\n",
        "print(f\"Unique users: {df_viz['user_id'].nunique():,}\")\n",
        "print(f\"Unique businesses: {df_viz['gmap_id'].nunique():,}\")\n",
        "print(f\"\\nRating Statistics:\")\n",
        "print(df_viz['rating'].describe())\n",
        "\n",
        "# Calculate text length\n",
        "df_viz['text_length'] = df_viz['text'].astype(str).apply(len)\n",
        "print(f\"\\nText Length Statistics:\")\n",
        "print(df_viz['text_length'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRsE6IsMFtzR"
      },
      "source": [
        "## A. Rating Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "D0ue63QfFtzS",
        "outputId": "9fce58fd-e3c6-4a7b-8715-9ffa9b788ba5"
      },
      "outputs": [],
      "source": [
        "# Rating distribution\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "\n",
        "# Bar plot with percentages\n",
        "rating_counts = df_viz['rating'].value_counts().sort_index()\n",
        "rating_pct = (rating_counts / len(df_viz) * 100).round(1)\n",
        "bars = ax.bar(rating_counts.index, rating_counts.values, edgecolor='black', alpha=0.7, color='coral')\n",
        "ax.set_xlabel('Rating', fontweight='bold')\n",
        "ax.set_ylabel('Count', fontweight='bold')\n",
        "ax.set_title('Rating Distribution', fontweight='bold')\n",
        "ax.set_xticks([1, 2, 3, 4, 5])\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add percentage labels on bars\n",
        "for idx, val in zip(rating_counts.index, rating_counts.values):\n",
        "    ax.text(idx, val, f'{rating_pct[idx]:.1f}%',\n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rating_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nRating Distribution:\")\n",
        "for rating in sorted(rating_counts.index):\n",
        "    print(f\"  Rating {int(rating)}: {rating_counts[rating]:,} ({rating_pct[rating]:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIOAz8o2FtzS"
      },
      "source": [
        "## B. Text Length Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Sr19r1enFtzT",
        "outputId": "0accdc5b-bf18-44e7-96fd-8ad6359fb105"
      },
      "outputs": [],
      "source": [
        "# Text length distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(df_viz['text_length'], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[0].set_xlabel('Text Length (characters)', fontweight='bold')\n",
        "axes[0].set_ylabel('Frequency', fontweight='bold')\n",
        "axes[0].set_title('Text Length Distribution', fontweight='bold')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Rating vs Text Length\n",
        "rating_text_length = df_viz.groupby('rating')['text_length'].mean()\n",
        "bars = axes[1].bar(rating_text_length.index, rating_text_length.values,\n",
        "                   edgecolor='black', alpha=0.7, color='mediumpurple')\n",
        "axes[1].set_xlabel('Rating', fontweight='bold')\n",
        "axes[1].set_ylabel('Average Text Length (characters)', fontweight='bold')\n",
        "axes[1].set_title('Average Text Length by Rating', fontweight='bold')\n",
        "axes[1].set_xticks([1, 2, 3, 4, 5])\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for idx, val in zip(rating_text_length.index, rating_text_length.values):\n",
        "    axes[1].text(idx, val, f'{int(val)}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('text_length_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAverage Text Length by Rating:\")\n",
        "for rating in sorted(rating_text_length.index):\n",
        "    print(f\"  Rating {int(rating)}: {rating_text_length[rating]:.1f} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8ybLCDNFtzU"
      },
      "source": [
        "## C. Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wu08qFeAFtzU",
        "outputId": "e4937f45-f6d4-4c0f-bf4d-94499cc3a8d8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Feature Engineering for Correlation Analysis\n",
        "# ============================================================================\n",
        "df_corr = df_viz.copy()\n",
        "\n",
        "# --- Feature 1: Text Length ---\n",
        "df_corr['text_length'] = df_corr['text'].astype(str).str.len()\n",
        "\n",
        "# --- Feature 2: Hours Count (number of open days per week) ---\n",
        "def count_open_days(hours_str):\n",
        "    \"\"\"Count the number of days a business is open per week.\n",
        "\n",
        "    Args:\n",
        "        hours_str: String like \"[[Thursday, 8AM–5PM], [Friday, 8AM–5PM], ...]\"\n",
        "                   or already parsed list\n",
        "\n",
        "    Returns:\n",
        "        Number of days the business is open (0-7)\n",
        "    \"\"\"\n",
        "    if pd.isna(hours_str):\n",
        "        return 0\n",
        "\n",
        "    # Parse hours string to list if it's a string\n",
        "    try:\n",
        "        if isinstance(hours_str, str):\n",
        "            # Try to parse as Python literal (list of lists)\n",
        "            hours_list = ast.literal_eval(hours_str)\n",
        "        else:\n",
        "            # Already a list\n",
        "            hours_list = hours_str\n",
        "\n",
        "        # Validate that we got a list\n",
        "        if not isinstance(hours_list, list):\n",
        "            return 0\n",
        "\n",
        "        # Count days that are not 'Closed'\n",
        "        # Format: [[DayName, Hours], [DayName, Hours], ...]\n",
        "        # Example: [[Thursday, 8AM–5PM], [Friday, 8AM–5PM], ...]\n",
        "        open_count = 0\n",
        "        for day_info in hours_list:\n",
        "            # day_info should be a list like [DayName, Hours]\n",
        "            if isinstance(day_info, list) and len(day_info) >= 2:\n",
        "                # Check if the hours part (index 1) is not 'Closed'\n",
        "                hours_value = day_info[1]\n",
        "                if hours_value != 'Closed' and hours_value is not None:\n",
        "                    open_count += 1\n",
        "\n",
        "        return open_count\n",
        "    except (ValueError, SyntaxError, TypeError) as e:\n",
        "        # If parsing fails, return 0\n",
        "        return 0\n",
        "\n",
        "df_corr['hours_count'] = df_corr['hours'].apply(count_open_days)\n",
        "\n",
        "# --- Feature 3: Price (converted to numerical) ---\n",
        "if 'price' in df_corr.columns:\n",
        "    price_map = {None: 0, np.nan: 0, '$': 1, '$$': 2, '$$$': 3, '$$$$': 4}\n",
        "    df_corr['price_numeric'] = df_corr['price'].map(price_map).fillna(0)\n",
        "else:\n",
        "    df_corr['price_numeric'] = 0\n",
        "\n",
        "# --- Feature 4: Average Rating (excluding current review) ---\n",
        "# Calculate avg_rating excluding current review to avoid data leakage\n",
        "business_stats = df_corr.groupby('gmap_id')['rating'].agg(['sum', 'count']).reset_index()\n",
        "business_stats.columns = ['gmap_id', 'total_rating', 'count']\n",
        "\n",
        "df_corr = df_corr.merge(business_stats, on='gmap_id', how='left')\n",
        "df_corr['avg_rating'] = (df_corr['total_rating'] - df_corr['rating']) / (df_corr['count'] - 1)\n",
        "# Handle businesses with only one review\n",
        "# Not needed in 10 core\n",
        "# df_corr.loc[df_corr['count'] == 1, 'avg_rating'] = df_corr.loc[df_corr['count'] == 1, 'rating']\n",
        "# df_corr = df_corr.drop(columns=['total_rating', 'count'])\n",
        "\n",
        "# --- Feature 5: Category Count (number of categories per business) ---\n",
        "def count_categories(cat_str):\n",
        "    \"\"\"Count the number of categories for a business.\"\"\"\n",
        "    if pd.isna(cat_str):\n",
        "        return 0\n",
        "    try:\n",
        "        if isinstance(cat_str, str):\n",
        "            cat_list = ast.literal_eval(cat_str)\n",
        "        else:\n",
        "            cat_list = cat_str\n",
        "        return len(cat_list) if isinstance(cat_list, list) else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "if 'category' in df_corr.columns:\n",
        "    df_corr['category_count'] = df_corr['category'].apply(count_categories)\n",
        "else:\n",
        "    df_corr['category_count'] = 0\n",
        "\n",
        "# --- Feature 6: Number of Reviews ---\n",
        "if 'num_of_reviews' in df_corr.columns:\n",
        "    df_corr['num_of_reviews'] = pd.to_numeric(df_corr['num_of_reviews'], errors='raise')\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Correlation Analysis\n",
        "# ============================================================================\n",
        "# Select numerical features for correlation analysis\n",
        "corr_features = ['rating', 'text_length', 'hours_count', 'price_numeric',\n",
        "                 'avg_rating', 'category_count', 'num_of_reviews']\n",
        "df_corr_subset = df_corr[corr_features].dropna()\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df_corr_subset.corr()\n",
        "\n",
        "# ============================================================================\n",
        "# Visualization\n",
        "# ============================================================================\n",
        "n_features = len(corr_matrix)\n",
        "fig_size = max(10, n_features * 1.2)\n",
        "\n",
        "plt.figure(figsize=(fig_size, fig_size))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=True,\n",
        "    fmt='.3f',\n",
        "    cmap='coolwarm',\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=1.5,\n",
        "    cbar_kws={\"shrink\": 0.8},\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        "    annot_kws={\n",
        "        'fontsize': max(10, int(14 - n_features * 0.5)),\n",
        "        'fontweight': 'bold'\n",
        "    },\n",
        "    xticklabels=corr_matrix.columns,\n",
        "    yticklabels=corr_matrix.columns\n",
        ")\n",
        "plt.title('Correlation Heatmap of Features', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# Results Summary\n",
        "# ============================================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Correlation Matrix:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(corr_matrix.round(3))\n",
        "\n",
        "# Key correlations with rating\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Correlation with Rating (Target Variable):\")\n",
        "print(f\"{'='*60}\")\n",
        "rating_corrs = corr_matrix['rating'].sort_values(ascending=False)\n",
        "for feature, corr_value in rating_corrs.items():\n",
        "    if feature != 'rating':\n",
        "        print(f\"  {feature:15s}: {corr_value:6.3f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEQsBpkYFtzV"
      },
      "source": [
        "# III. Modeling\n",
        "\n",
        "This section describes the models we implement for rating prediction. We start with simple baselines and progressively build more sophisticated models that incorporate text features and metadata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Z1cUfMFtzV"
      },
      "source": [
        "## A. Baseline Models\n",
        "\n",
        "We implement three baseline models to establish a performance floor:\n",
        "\n",
        "1. **Global Mean**: Predicts the average rating across all training examples\n",
        "2. **User Means**: Predicts the user's average rating across all training examples.\n",
        "3. **Item Means**: Predicts the item's average rating across all training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9qeqvlLFtzV",
        "outputId": "6de1c438-aec5-4b79-a3c5-89fcad765ac2"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Data Loading and Preparation\n",
        "# ============================================================================\n",
        "df = pd.read_csv(\"merged.csv\", usecols=['user_id', 'rating', 'gmap_id'])\n",
        "df = df.dropna(subset=['user_id', 'rating', 'gmap_id'])\n",
        "df['rating'] = df['rating'].astype(float)\n",
        "\n",
        "# ============================================================================\n",
        "# Train/Test Split (80/20)\n",
        "# ============================================================================\n",
        "train, test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['rating']\n",
        ")\n",
        "\n",
        "print(f\"\\nData Split:\")\n",
        "print(f\"  Train: {len(train):,} samples ({len(train)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Test:  {len(test):,} samples ({len(test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# Baseline 1: Global Mean\n",
        "# ============================================================================\n",
        "# Predict the average rating across all training examples\n",
        "global_mean = train['rating'].mean()\n",
        "preds_global = [global_mean] * len(test)\n",
        "mse_global = mean_squared_error(test['rating'], preds_global)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Baseline 1: Global Mean\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Prediction: {global_mean:.4f} (constant)\")\n",
        "print(f\"Test MSE:   {mse_global:.4f}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Baseline 2: User Means\n",
        "# ============================================================================\n",
        "# Predict the average rating the user gives across all training examples\n",
        "# If the user isn't found in the training data, use the global average instead\n",
        "user_means = train.groupby('user_id')['rating'].mean()\n",
        "preds_user = test['user_id'].map(user_means)\n",
        "preds_user = preds_user.fillna(global_mean)\n",
        "mse_user = mean_squared_error(test['rating'], preds_user)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Baseline 2: User Means\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Test MSE:   {mse_user:.4f}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Baseline 3: Item Means\n",
        "# ============================================================================\n",
        "# Predict the average rating of the item\n",
        "# If the item isn't found in the training data, use the global average instead\n",
        "item_means = train.groupby('gmap_id')['rating'].mean()\n",
        "preds_item = test['gmap_id'].map(item_means)\n",
        "preds_item = preds_item.fillna(global_mean)\n",
        "\n",
        "mse_item = mean_squared_error(test['rating'], preds_item)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Baseline 3: Item Means\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Test MSE:   {mse_item:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enx0OIHhFtzV"
      },
      "source": [
        "## B. Text-based Linear Model (Main Model #1)\n",
        "\n",
        "**Intuition**: The sentiment and expressions in text are most directly connected to ratings. Words, phrases, and sentiment expressions contained in review text provide key information for predicting the ratings assigned by users.\n",
        "\n",
        "We use:\n",
        "- **TF-IDF vectorization** to convert text into numerical features\n",
        "- **Ridge Regression** for regularization and to handle the high-dimensional feature space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snTq_O4zFtzV",
        "outputId": "ac507c1d-816b-425b-d030-bdad43010d3c"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Data Loading and Preparation\n",
        "# ============================================================================\n",
        "df = pd.read_csv(\"merged.csv\", usecols=['text', 'rating'])\n",
        "df = df.dropna(subset=['text', 'rating'])\n",
        "df['text'] = df['text'].astype(str)\n",
        "df['rating'] = df['rating'].astype(float)\n",
        "\n",
        "# ============================================================================\n",
        "# Train/Validation/Test Split (70/15/15)\n",
        "# ============================================================================\n",
        "# Step 1: Separate test set (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    df['text'], df['rating'],\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=df['rating']\n",
        ")\n",
        "\n",
        "# Step 2: Split remaining 85% into train (70%) and validation (15%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.15/0.85,  # ~17.6% of 85% = 15% of total\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"\\nData Split:\")\n",
        "print(f\"  Train:      {len(X_train):,} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(X_val):,} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Test:       {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# Feature Engineering: TF-IDF Vectorization\n",
        "# ============================================================================\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=50000,\n",
        "    ngram_range=(1, 2),    # Unigrams and bigrams\n",
        "    min_df=2,              # Ignore terms in < 2 documents\n",
        "    max_df=0.95            # Ignore terms in > 95% of documents\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf.transform(X_val)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(f\"\\nTF-IDF Features:\")\n",
        "print(f\"  Shape: {X_train_tfidf.shape}\")\n",
        "print(f\"  Vocabulary size: {len(tfidf.vocabulary_):,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Hyperparameter Tuning on Validation Set\n",
        "# ============================================================================\n",
        "alpha_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "best_alpha = 1.0\n",
        "best_val_mse = float('inf')\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Hyperparameter Tuning (Validation Set)\")\n",
        "print(f\"{'='*60}\")\n",
        "for alpha in alpha_values:\n",
        "    model_temp = Ridge(alpha=alpha)\n",
        "    model_temp.fit(X_train_tfidf, y_train)\n",
        "    y_pred_val = model_temp.predict(X_val_tfidf)\n",
        "    val_mse = mean_squared_error(y_val, y_pred_val)\n",
        "    print(f\"  Alpha = {alpha:4.1f}:  Validation MSE = {val_mse:.4f}\")\n",
        "    if val_mse < best_val_mse:\n",
        "        best_val_mse = val_mse\n",
        "        best_alpha = alpha\n",
        "\n",
        "print(f\"\\nBest hyperparameters:\")\n",
        "print(f\"  Alpha = {best_alpha:.1f}\")\n",
        "print(f\"  Validation MSE = {best_val_mse:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Final Model Training and Evaluation\n",
        "# ============================================================================\n",
        "# Train on full training set with best hyperparameters\n",
        "model_text = Ridge(alpha=best_alpha)\n",
        "model_text.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate on test set (final evaluation only)\n",
        "y_pred_text = model_text.predict(X_test_tfidf)\n",
        "mse_text = mean_squared_error(y_test, y_pred_text)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Final Model Performance (Test Set)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Model: Text-based (TF-IDF + Ridge)\")\n",
        "print(f\"Alpha: {best_alpha:.1f}\")\n",
        "print(f\"Test MSE: {mse_text:.4f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwR7lzyfFtzW"
      },
      "source": [
        "## C. Text + Metadata Model\n",
        "\n",
        "This model combines text features with metadata to capture both semantic content and contextual information about the business.\n",
        "\n",
        "**Features**:\n",
        "- **TF-IDF(text)**: Text vectorization\n",
        "- **avg_rating**: Average rating of the business (excluding current review) - high correlation feature\n",
        "- **text_length**: Length of review text - correlation feature\n",
        "\n",
        "We use **Ridge Regression** for this model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-3Jy-TdFtzW",
        "outputId": "a2aa07b8-4236-4c29-bb04-e27c2abb68b3"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Data Loading and Preparation\n",
        "# ============================================================================\n",
        "df = pd.read_csv(\"merged.csv\", usecols=['text', 'rating', 'gmap_id'])\n",
        "df = df.dropna(subset=['text', 'rating', 'gmap_id'])\n",
        "df['text'] = df['text'].astype(str)\n",
        "df['rating'] = df['rating'].astype(float)\n",
        "\n",
        "# ============================================================================\n",
        "# Train/Validation/Test Split (70/15/15)\n",
        "# ============================================================================\n",
        "# Step 1: Separate test set (15%)\n",
        "df_temp, df_test = train_test_split(\n",
        "    df,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=df['rating']\n",
        ")\n",
        "\n",
        "# Step 2: Split remaining 85% into train (70%) and validation (15%)\n",
        "df_train, df_val = train_test_split(\n",
        "    df_temp,\n",
        "    test_size=0.15/0.85,  # ~17.6% of 85% = 15% of total\n",
        "    random_state=42,\n",
        "    stratify=df_temp['rating']\n",
        ")\n",
        "\n",
        "print(f\"\\nData Split:\")\n",
        "print(f\"  Train:      {len(df_train):,} samples ({len(df_train)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(df_val):,} samples ({len(df_val)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Test:       {len(df_test):,} samples ({len(df_test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# Feature Engineering\n",
        "# ============================================================================\n",
        "\n",
        "# --- Feature 1: TF-IDF Vectorization ---\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=50000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.95\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(df_train['text'])\n",
        "X_val_tfidf = tfidf.transform(df_val['text'])\n",
        "X_test_tfidf = tfidf.transform(df_test['text'])\n",
        "\n",
        "print(f\"\\nTF-IDF Features:\")\n",
        "print(f\"  Shape: {X_train_tfidf.shape}\")\n",
        "\n",
        "# --- Feature 2: Average Rating (excluding current review) ---\n",
        "# Calculate business statistics from train set only (avoid data leakage)\n",
        "business_stats = df_train.groupby('gmap_id')['rating'].agg(['sum', 'count']).reset_index()\n",
        "business_stats.columns = ['gmap_id', 'total_rating', 'count']\n",
        "\n",
        "# Train set: exclude current review\n",
        "df_train_merged = df_train.merge(business_stats, on='gmap_id', how='left')\n",
        "df_train_merged['avg_rating'] = (df_train_merged['total_rating'] - df_train_merged['rating']) / (df_train_merged['count'] - 1)\n",
        "df_train_merged.loc[df_train_merged['count'] == 1, 'avg_rating'] = df_train_merged.loc[df_train_merged['count'] == 1, 'rating']\n",
        "avg_rating_train = df_train_merged['avg_rating'].fillna(df_train['rating'].mean()).values\n",
        "\n",
        "# Validation/Test sets: use train statistics only\n",
        "df_val_merged = df_val.merge(business_stats, on='gmap_id', how='left')\n",
        "df_val_merged['avg_rating'] = df_val_merged['total_rating'] / df_val_merged['count']\n",
        "avg_rating_val = df_val_merged['avg_rating'].fillna(df_train['rating'].mean()).values\n",
        "\n",
        "df_test_merged = df_test.merge(business_stats, on='gmap_id', how='left')\n",
        "df_test_merged['avg_rating'] = df_test_merged['total_rating'] / df_test_merged['count']\n",
        "avg_rating_test = df_test_merged['avg_rating'].fillna(df_train['rating'].mean()).values\n",
        "\n",
        "# --- Feature 3: Text Length ---\n",
        "text_length_train = df_train['text'].str.len().values\n",
        "text_length_val = df_val['text'].str.len().values\n",
        "text_length_test = df_test['text'].str.len().values\n",
        "\n",
        "print(f\"\\nMetadata Features:\")\n",
        "print(f\"  Avg Rating - Train: {np.mean(avg_rating_train):.2f}, Val: {np.mean(avg_rating_val):.2f}, Test: {np.mean(avg_rating_test):.2f}\")\n",
        "print(f\"  Text Length - Train: {np.mean(text_length_train):.1f}, Val: {np.mean(text_length_val):.1f}, Test: {np.mean(text_length_test):.1f}\")\n",
        "\n",
        "# --- Combine and Scale Features ---\n",
        "metadata_train = np.column_stack([avg_rating_train, text_length_train])\n",
        "metadata_val = np.column_stack([avg_rating_val, text_length_val])\n",
        "metadata_test = np.column_stack([avg_rating_test, text_length_test])\n",
        "\n",
        "# Scale metadata features (fit only on train to avoid data leakage)\n",
        "scaler = StandardScaler()\n",
        "metadata_train_scaled = scaler.fit_transform(metadata_train)\n",
        "metadata_val_scaled = scaler.transform(metadata_val)\n",
        "metadata_test_scaled = scaler.transform(metadata_test)\n",
        "\n",
        "# Combine TF-IDF (sparse) + metadata (dense)\n",
        "X_train_combined = hstack([X_train_tfidf, metadata_train_scaled])\n",
        "X_val_combined = hstack([X_val_tfidf, metadata_val_scaled])\n",
        "X_test_combined = hstack([X_test_tfidf, metadata_test_scaled])\n",
        "\n",
        "print(f\"\\nCombined Features:\")\n",
        "print(f\"  Train: {X_train_combined.shape}\")\n",
        "print(f\"  Val:   {X_val_combined.shape}\")\n",
        "print(f\"  Test:  {X_test_combined.shape}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Hyperparameter Tuning on Validation Set\n",
        "# ============================================================================\n",
        "alpha_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "best_alpha = 1.0\n",
        "best_val_mse = float('inf')\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Hyperparameter Tuning (Validation Set)\")\n",
        "print(f\"{'='*60}\")\n",
        "for alpha in alpha_values:\n",
        "    model_temp = Ridge(alpha=alpha)\n",
        "    model_temp.fit(X_train_combined, df_train['rating'])\n",
        "    y_pred_val = model_temp.predict(X_val_combined)\n",
        "    val_mse = mean_squared_error(df_val['rating'], y_pred_val)\n",
        "    print(f\"  Alpha = {alpha:4.1f}:  Validation MSE = {val_mse:.4f}\")\n",
        "    if val_mse < best_val_mse:\n",
        "        best_val_mse = val_mse\n",
        "        best_alpha = alpha\n",
        "\n",
        "print(f\"\\nBest hyperparameters:\")\n",
        "print(f\"  Alpha = {best_alpha:.1f}\")\n",
        "print(f\"  Validation MSE = {best_val_mse:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Final Model Training and Evaluation\n",
        "# ============================================================================\n",
        "# Train on full training set with best hyperparameters\n",
        "model_ridge = Ridge(alpha=best_alpha)\n",
        "model_ridge.fit(X_train_combined, df_train['rating'])\n",
        "\n",
        "# Evaluate on test set (final evaluation only)\n",
        "y_pred_ridge = model_ridge.predict(X_test_combined)\n",
        "mse_ridge = mean_squared_error(df_test['rating'], y_pred_ridge)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Final Model Performance (Test Set)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Model: Text + Metadata (TF-IDF + avg_rating + text_length + Ridge)\")\n",
        "print(f\"Alpha: {best_alpha:.1f}\")\n",
        "print(f\"Test MSE: {mse_ridge:.4f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3TupgecFtzW"
      },
      "source": [
        "## D. LinearSVR Model\n",
        "\n",
        "**Explanation**: Linear Support Vector Regression (LinearSVR) is a linear model that uses support vector machines for regression tasks. It can handle high-dimensional sparse data well and often performs better than Ridge regression for certain types of text classification problems.\n",
        "\n",
        "We use **LinearSVR** with TF-IDF features for rating prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqkENxeoFtzW",
        "outputId": "cb9a9499-67a1-4494-acec-08745cae8bae"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Data Loading and Preparation\n",
        "# ============================================================================\n",
        "df = pd.read_csv(\"merged.csv\", usecols=['text', 'rating'])\n",
        "df = df.dropna(subset=['text', 'rating'])\n",
        "df['text'] = df['text'].astype(str)\n",
        "df['rating'] = df['rating'].astype(float)\n",
        "\n",
        "# ============================================================================\n",
        "# Train/Validation/Test Split (70/15/15)\n",
        "# ============================================================================\n",
        "# Step 1: Separate test set (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    df['text'], df['rating'],\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=df['rating']\n",
        ")\n",
        "\n",
        "# Step 2: Split remaining 85% into train (70%) and validation (15%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.15/0.85,\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"\\nData Split:\")\n",
        "print(f\"  Train:      {len(X_train):,} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(X_val):,} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Test:       {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# Feature Engineering: TF-IDF Vectorization\n",
        "# ============================================================================\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=50000,\n",
        "    ngram_range=(1, 2),    # Unigrams and bigrams\n",
        "    min_df=2,              # Ignore terms in < 2 documents\n",
        "    max_df=0.95            # Ignore terms in > 95% of documents\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf.transform(X_val)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(f\"\\nTF-IDF Features:\")\n",
        "print(f\"  Shape: {X_train_tfidf.shape}\")\n",
        "print(f\"  Vocabulary size: {len(tfidf.vocabulary_):,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Hyperparameter Tuning on Validation Set\n",
        "# ============================================================================\n",
        "# C: Regularization parameter (inverse of regularization strength)\n",
        "# epsilon: Epsilon-tube (no penalty within this range)\n",
        "C_values = [0.1, 0.5, 1.0, 2.0]\n",
        "epsilon_values = [0.0, 0.1, 0.2]\n",
        "best_C = 1.0\n",
        "best_epsilon = 0.0\n",
        "best_val_mse = float('inf')\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Hyperparameter Tuning (Validation Set)\")\n",
        "print(f\"{'='*60}\")\n",
        "for C in C_values:\n",
        "    for epsilon in epsilon_values:\n",
        "        model_temp = LinearSVR(C=C, epsilon=epsilon, max_iter=1000, random_state=42)\n",
        "        model_temp.fit(X_train_tfidf, y_train)\n",
        "        y_pred_val = model_temp.predict(X_val_tfidf)\n",
        "        y_pred_val = np.clip(y_pred_val, 1.0, 5.0)  # Clip to valid rating range\n",
        "        val_mse = mean_squared_error(y_val, y_pred_val)\n",
        "        print(f\"  C = {C:4.1f}, epsilon = {epsilon:.1f}:  Validation MSE = {val_mse:.4f}\")\n",
        "        if val_mse < best_val_mse:\n",
        "            best_val_mse = val_mse\n",
        "            best_C = C\n",
        "            best_epsilon = epsilon\n",
        "\n",
        "print(f\"\\nBest hyperparameters:\")\n",
        "print(f\"  C = {best_C:.1f}\")\n",
        "print(f\"  epsilon = {best_epsilon:.1f}\")\n",
        "print(f\"  Validation MSE = {best_val_mse:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Final Model Training and Evaluation\n",
        "# ============================================================================\n",
        "# Train on full training set with best hyperparameters\n",
        "model_svr = LinearSVR(C=best_C, epsilon=best_epsilon, max_iter=1000, random_state=42)\n",
        "model_svr.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate on test set (final evaluation only)\n",
        "y_pred_svr = model_svr.predict(X_test_tfidf)\n",
        "y_pred_svr = np.clip(y_pred_svr, 1.0, 5.0)  # Clip to valid rating range\n",
        "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Final Model Performance (Test Set)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Model: LinearSVR (TF-IDF + LinearSVR)\")\n",
        "print(f\"C: {best_C:.1f}, epsilon: {best_epsilon:.1f}\")\n",
        "print(f\"Test MSE: {mse_svr:.4f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E. BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Data Loading and Preparation\n",
        "# ============================================================================\n",
        "df = pd.read_csv(\"merged.csv\", usecols=['text', 'rating', 'gmap_id'])\n",
        "df = df.dropna(subset=['text', 'rating', 'gmap_id'])\n",
        "df['text'] = df['text'].astype(str)\n",
        "df['rating'] = df['rating'].astype(float)\n",
        "\n",
        "# ============================================================================\n",
        "# Train/Validation/Test Split (70/15/15)\n",
        "# ============================================================================\n",
        "# Step 1: Separate test set (15%)\n",
        "df_temp, df_test = train_test_split(\n",
        "    df, \n",
        "    test_size=0.15, \n",
        "    random_state=42, \n",
        "    stratify=df['rating']\n",
        ")\n",
        "\n",
        "# Step 2: Split remaining 85% into train (70%) and validation (15%)\n",
        "df_train, df_val = train_test_split(\n",
        "    df_temp, \n",
        "    test_size=0.15/0.85,  # ~17.6% of 85% = 15% of total\n",
        "    random_state=42, \n",
        "    stratify=df_temp['rating']\n",
        ")\n",
        "\n",
        "print(f\"\\nData Split:\")\n",
        "print(f\"  Train:      {len(df_train):,} samples ({len(df_train)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(df_val):,} samples ({len(df_val)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Test:       {len(df_test):,} samples ({len(df_test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# Tokenization and Dataset Preparation\n",
        "# ============================================================================\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments, default_data_collator\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "\n",
        "#tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "#encode\n",
        "train_encodings = tokenizer(df_train['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(df_val['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(df_test['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "#develop data set\n",
        "class RatingDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        # convert ratings 1–5 to 0–4\n",
        "        self.labels = [l - 1 for l in labels]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = RatingDataset(train_encodings, df_train['rating'].tolist())\n",
        "val_dataset = RatingDataset(val_encodings, df_val['rating'].tolist())\n",
        "test_dataset = RatingDataset(test_encodings, df_test['rating'].tolist())\n",
        "\n",
        "# ============================================================================\n",
        "# Model Initialization\n",
        "# ============================================================================\n",
        "\n",
        "num_labels = 5  # ratings 1–5\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# Training Arguments\n",
        "# ============================================================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_output\",\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=3e-5,   \n",
        "    weight_decay=0.01,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# the loss is cross entropy\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# ============================================================================\n",
        "# Final Model Evaluation\n",
        "# ============================================================================\n",
        "\n",
        "# save model so I don't have to train again\n",
        "model.save_pretrained(\"./bert_rating_model\")\n",
        "tokenizer.save_pretrained(\"./bert_rating_model\")\n",
        "print(\"Model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Final Model Evaluation/Part not repeat computation\n",
        "# ============================================================================\n",
        "\n",
        "model_path = \"./bert_rating_model\"\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()  # important for inference\n",
        "\n",
        "trainer = Trainer(model=model, data_collator=default_data_collator)\n",
        "\n",
        "# Make predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# logits -> class indices\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "# convert back to rating scale (1–5)\n",
        "y_pred = preds + 1  \n",
        "y_true = df_test['rating'].tolist()\n",
        "\n",
        "# Evaluate on test set (final evaluation only)\n",
        "mse_bert = mean_squared_error(y_true, y_pred)\n",
        "rmse_bert = np.sqrt(mse_bert)\n",
        "accuracy_bert = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Final Model Performance (Test Set)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Model: DistilBERT (Text → Rating)\")\n",
        "print(f\"Test MSE: {mse_bert:.4f}\")\n",
        "print(f\"Test RMSE: {rmse_bert:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_bert:.4f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKlx08cyFtzX"
      },
      "source": [
        "## F. Model Comparison\n",
        "\n",
        "Below is a comprehensive comparison of all models we implemented:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "LcXkKihVFtzX",
        "outputId": "91bcc423-7df0-4829-dbd4-97fab049179e"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Model Comparison\n",
        "# ============================================================================\n",
        "results = {\n",
        "    'Model': [\n",
        "        'Global Mean (Baseline)',\n",
        "        'User Means (Baseline)',\n",
        "        'Item Means (Baseline)',\n",
        "        'LinearSVR',\n",
        "        'Text-based (Ridge)',\n",
        "        'Text + Metadata (Ridge)',\n",
        "        'DistilBERT (Text → Rating)',\n",
        "    ],\n",
        "    'MSE': [\n",
        "        mse_global,\n",
        "        mse_user,\n",
        "        mse_item,\n",
        "        mse_svr,\n",
        "        mse_text,\n",
        "        mse_ridge,\n",
        "        mse_bert,\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(results)\n",
        "comparison_df = comparison_df.sort_values('MSE')\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"MODEL COMPARISON (Sorted by MSE - Lower is Better)\")\n",
        "print(f\"{'='*80}\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "495E_JJFFtzX",
        "outputId": "e1e1dfc0-bae1-4d2c-fcab-7a3711615176"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Model Performance Visualization\n",
        "# ============================================================================\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "# Sort by MSE (best to worst)\n",
        "comparison_sorted = comparison_df.sort_values('MSE')\n",
        "\n",
        "# Color gradient: green (best) to red (worst)\n",
        "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(comparison_sorted)))\n",
        "bars = ax.barh(\n",
        "    comparison_sorted['Model'],\n",
        "    comparison_sorted['MSE'],\n",
        "    color=colors,\n",
        "    edgecolor='black',\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Mean Squared Error (MSE)', fontweight='bold', fontsize=12)\n",
        "ax.set_title('Model Performance Comparison', fontweight='bold', fontsize=14)\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "ax.invert_yaxis()  # Best model on top\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (idx, val) in enumerate(zip(comparison_sorted.index, comparison_sorted['MSE'])):\n",
        "    ax.text(val, i, f' {val:.4f}', va='center', fontweight='bold', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
